{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRYEofSD0xoF"
   },
   "source": [
    "# Perceptrón multicapa\n",
    "\n",
    "Consideraremos un perceptrón multicapa, con capas enumeradas por $l=0,1,...,L$.\n",
    "Denotemos por $x^l_i$ el estado de la $i$-ésima neurona en la capa $l$.\n",
    "Diremos que la red posee $n^l$ neuronas $i=1,...,n^l$ en la $l$-ésima capa.\n",
    "En particular, $x^0$ denota el vector de estados de la capa de entrada y $x^L$ el vector de estados de la capa de salida.\n",
    "Se tiene que\n",
    "\\begin{equation}\n",
    "x^l_i\n",
    "=\n",
    "g(h^l_i)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (1)\n",
    "\\end{equation}\n",
    "donde $g:\\mathbb{R}\\to \\mathbb{R}$ es una función de activación, por ejemplo una sigmoide $g(h)=1/(1+e^{-h})$, y\n",
    "\\begin{equation}\n",
    "h^{l}_i\n",
    "=\n",
    "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (2)\n",
    "\\end{equation}\n",
    "es el campo local sufrido por la $i$-ésima neurona en la $l$-ésima capa .\n",
    "Además, $w^l_{ij}$ denota la intensidad de la sinapsis que conecta la $j$-ésima neurona en la $(l-1)$-ésima capa con la $i$-ésima neurona en la $l$-ésima capa.\n",
    "Notar, la red depende de las matrices de pesos sinápticos $w^1,w^2,...,w^{L}$.\n",
    "\n",
    "## Umbrales de activación\n",
    "\n",
    "En cada una de las capas $l=0,1,...,L-1$, se agrega una neurona extra $i=n^l+1$ con un estado fijo $x^l_{n^l+1}=-1$.\n",
    "De esta manera, una nueva sinapsis $u^{l}_i:=w^{l}_{i,n^{l-1}+1}$ hace las veces de umbral de activación de la $i$-ésima neurona en la $l$-ésima capa, ya que\n",
    "\\begin{equation}\n",
    "h^{l+1}_i\n",
    "=\n",
    "w^{l+1}_{i,n^{l}+1} x^{l}_{n^{l}+1}\n",
    "+\n",
    "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^{l}_j\n",
    "=\n",
    "-\n",
    "u^{l+1}_i\n",
    "+\n",
    "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^l_j\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (3)\n",
    "\\end{equation}\n",
    "\n",
    "## Conjunto de entrenamiento\n",
    "\n",
    "Los datos de entrenamiento consisten en un conjunto de pares $\\{(e^m,s^m):m=1,...,M\\}$ donde $e^m\\in \\mathbb{R}^{n_0}$ y $s^m\\in \\mathbb{R}^{n_L}$ son vectores que representan el $m$-ésimo par de entrada-salida o *ejemplo* que debe aprender la red.\n",
    "\n",
    "## Función costo: el Error Cuadrático\n",
    "\n",
    "Si pensamos que la salida de la red es una función de la entrada, i.e. que $x^L(x^0)$, podemos evaluar el error que comete la red sobre el conjunto de entramiento utilizando el *error cuadrático*\n",
    "$$\n",
    "E\n",
    "=\n",
    "\\sum_{m=1}^M F^m\n",
    "$$\n",
    "como *función costo*, donde\n",
    "$$\n",
    "F^m\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\sum_{i=1}^{n^L}\n",
    "(x^L_i(x^0=e^m) - s^m_i)^2\n",
    "$$\n",
    "es el error cuadrático que comete la red sobre el $m$-ésimo ejemplo.\n",
    "\n",
    "## Entrenamiento: descenso por el gradiente\n",
    "\n",
    "Entrenar la red consisten en encontrar valores de los pesos sinápticos $w^l_{ij}$ que minimicen el error $E$.\n",
    "Para ello, expresamos el error en función de dichos pesos y calculamos las componentes de su gradiente\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^l_{ij}}\n",
    "=\n",
    "\\sum_m\n",
    "\\frac{\\partial F^m}{\\partial w^l_{ij}}\n",
    "$$\n",
    "De esta manera, podemos utilizar el algoritmo de descenso por el gradiente para actualizar los pesos hasta que el error alcance un mínimo global.\n",
    "Más precisamente, partiendo de valores aleatorios\n",
    "$(w^l_{ij})^0$ para los pesos sinápticos, actualizamos iterativamente a los mismos con la siguiente regla\n",
    "\\begin{equation}\n",
    "(w^l_{ij})^{t+1} = (w^l_{ij})^t-\\eta \\frac{\\partial F^m}{\\partial w^l_{ij}}((w^l_{ij})^t)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (4)\n",
    "\\end{equation}\n",
    "para todo $l$, $ij$ y $m$, donde el parámetro $0<\\eta\\ll 1$ controla la tasa de aprendizaje.\n",
    "La iteración se detiene cuando ya no se advierten reducciones significativas del error $E$.\n",
    "\n",
    "## Cálculo del gradiente del error cuadrático\n",
    "\n",
    "Con el fin de simplificar la notación, elegimos un valor arbitrario de $m$ y obviamos la dependencia de las expresiones con éste índice.\n",
    "\n",
    "Notar que los vectores $x^l$ y $h^l$ sólo dependen de las matrices $w^1,...,w^{l}$.\n",
    "De esta manera, observamos que\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "g'(h^l_i)\n",
    "\\frac{\\partial h^l_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "si $r\\leq l$, y\n",
    "$$\n",
    "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}=0\n",
    "$$\n",
    "en caso contrario.\n",
    "Por otro lado,\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial h^{l}_i}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\frac{\\partial}{\\partial w^r_{pq}}\n",
    "\\bigg(\n",
    "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
    "\\bigg)\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j w^{l}_{ij}\n",
    "\\frac{\\partial x^{l-1}_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "si $r<l$, y\n",
    "$$\n",
    "\\frac{\\partial h^l_i}{\\partial w^{l}_{pq}}\n",
    "=\n",
    "\\sum_j\n",
    "\\delta_{ip}\n",
    "\\delta_{jq}\n",
    "x^{l-1}_j\n",
    "=\n",
    "\\delta_{ip}\n",
    "x^{l-1}_q\n",
    "$$\n",
    "Con estas ecuaciones se pueden establecer una relación de recurrencia que nos permite calcular las componentes del gradiente de $F$.\n",
    "A saber\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_i (x^L_i-s_i)\n",
    "\\frac{\\partial x^L}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i (x^L_i-s_i)\n",
    "g'(h^L_i)\n",
    "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\sum_j\n",
    "w^L_{ij}\n",
    "\\frac{\\partial x^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\sum_j\n",
    "w^L_{ij}\n",
    "g'(h^{L-1}_j)\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "\\bigg(\n",
    "g'(h^{L-1}_j)\n",
    "\\sum_i\n",
    "w^L_{ij}\n",
    "D^L_i\n",
    "\\bigg)\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "D^{L-1}_j\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "donde\n",
    "\\begin{equation}\n",
    "D^L_i:=(x^L_i-s_i)g'(h^L_i)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (5)\n",
    "\\end{equation}\n",
    "y\n",
    "$$\n",
    "D^{L-1}_j\n",
    ":=\n",
    "g'(h^{L-1}_j)\n",
    "\\sum_i\n",
    "w^L_{ij}\n",
    "D^L_i\n",
    "$$\n",
    "representan los *errores locales* de la $i$-ésima neurona en la $L$-ésima capa y la $j$-ésima neurona en la $(L-1)$-ésima capa, respectivamente.\n",
    "\n",
    "El anterior procedimiento puede continuarse capa por capa, con cada capa $l$ tal que $r<l$, de manera que\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_j D_j^l\n",
    "\\frac{\\partial h^l_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "donde\n",
    "\\begin{equation}\n",
    "D_j^l\n",
    ":=\n",
    "g'(h^{l}_j)\n",
    "\\sum_i w^{l+1}_{ij}D_i^{l+1}\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (6)\n",
    "\\end{equation}\n",
    "hasta que eventualmente se alcanza la capa $l=r$, y se obtiene\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_j\n",
    "D_j^{r}\n",
    "\\frac{\\partial h^{r}_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "D_j^{r}\n",
    "\\delta_{jp}\n",
    "x^{r-1}_q\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "D_p^{r}\n",
    "x^{r-1}_q\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "En particular, este último resultado se verifica para el caso $r=L$ de $pq$ arbitrario.\n",
    "También se verifica para el caso en que $q=n^{r-1}+1$ y valores arbitrarios de $r$ y $p$, en donde $x_q^{r-1}=-1$ corresponde al estado fijo de la neurona en la capa $(r-1)$-ésima que permite simular la acción de umbrales en la capa $r$-ésima, tal como se describe en la Ec. 3.\n",
    "\n",
    "## El algoritmo de backpropagation\n",
    "\n",
    "Los resultados anteriores pueden condensarse en el llamado *algoritmo de backpropagation*, el cuál permite el cálculo del gradiente y la actualización de los pesos sinápticos, y consiste en la siguiente lista de pasos.\n",
    "Para cada ejemplo $m=1,...,M$, ejecutar:\n",
    "1. *Forward pass:* calcular la salida $x^L$ de la red ante la entrada $x^1=e^m$ utilizando las Ecs. 1 y 2. En el proceso, guardar los valores de activación $x^l$ y de los correspondientes campos locales $h^l$ obtenidos en las distintas capas $l=2,...,L$, ya que serán útiles más adelante.\n",
    "2. Calcular el vector de errores $D^L$ de la capa de salida utilizando la Ec. 5.\n",
    "3. Propagar los errores hacia atrás, i.e. calcular los errores $D^l$ para $l=L-1,L-2,...,1$ utilizando la Ec. 6.\n",
    "4. Para cada $l$, $i$ y $j$, calcular el gradiente $\\frac{\\partial F^m}{\\partial w^l_{ij}}$ utilizando la Ec. 7 y actualizar el correspondiente peso sináptico $w^l_{ij}$ utilizando la Ec. 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XeWtDvx6C8A"
   },
   "source": [
    "## **Ejercicio 1**\n",
    "\n",
    "Genere un conjunto de entrenamiento compuesto por $M=\\sum_c m_c$ puntos en $\\mathbb{R}^{n_e}$ distribuidos en $n_s$ nubes de $m_c$ puntos.\n",
    "\n",
    "Para generar las nubes:\n",
    "\n",
    "* genere aleatoriamente $n_s$ puntos en $\\mathbb{R}^{n_e}$ a los que llamaremos centros, sorteando los valores de las coordenadas a partir de una distribución normal, y\n",
    "\n",
    "* para cada centro $c$, genere $m_c$ puntos aleatorios alrededor del mismo, sumando sus coordenadas a números aleatorios generados con una Gaussiana de varianza $\\sigma^2$.\n",
    "\n",
    "Las $n_e$ coordenadas del $m$-ésimo punto constituirán el vector de entrada del $m$-ésimo ejemplo.\n",
    "La nube a la que pertenece el $m$-ésimo punto determinará el vector de salida del $m$-ésimo ejemplo.\n",
    "Más precisamente, si el $m$-ésimo punto pertenence a la $c$-ésima nube, el vector de salida será el vector canónico $(0,0,...,1,...,0)$ de $n_s$ componentes con un único 1 en la $c$-esima posición.\n",
    "\n",
    "Concretamente\n",
    "\n",
    "1. Genere un conjunto de 8 puntos en $\\mathbb{R}^{n_e}$ con $n_e=2$, divididos en 3 nubes con $m_1=3$ en la primera nube, $m_2=2$ puntos en la segunda nube y $m_3=3$ puntos en la tercera nube. Utilice $\\sigma=0.1$ para indicar la dispersión de los puntos alrededor de cada nube.\n",
    "\n",
    "2. Grafique las nubes de puntos, utilizando un color distinto para cada una de ellas.\n",
    "\n",
    "## **Ejercicio 2**\n",
    "\n",
    "1. Implemente un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, una capa oculta de $n_o=2$ neuronas, y una capa de salida de $n_s=3$ neuronas. Recuerde, además, agregar las neuroas auxiliares que se utiliza para imitar los umbrales de activación. Utilice funciones de activación **sigmoideas**.\n",
    "\n",
    "2. Entrenelo sobre el conjunto de ejemplos generado en el Ejercicio 1. Para entrenarlo, utilice una tasa $\\eta=0.02$ y alrededor de 10.000 de épocas o más, según considere necesario.\n",
    "\n",
    "3. Grafique el error $E$ en función del número de épocas de entrenamiento.\n",
    "\n",
    "4. Luego, grafique nuevamente los puntos del Ejercicio 1, pintando el relleno de los mismos con los colores correspondiente a cada nube, y el borde de los mismos con el color correspondiente a la predicción obtenida con el **perceptrón multicapa**. Coinciden las predicciones con los colores originales?\n",
    "\n",
    "5. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?\n",
    "\n",
    "## **Ejercicio 3: la compuerta XOR**\n",
    "\n",
    "1. Fabrique un dataset con el siguiente conjunto de 4 ejemplos:\n",
    "\n",
    "    * $e_1 = (0,0)$, $s_1=(1,0)$\n",
    "    * $e_2 = (0,1)$, $s_2=(0,1)$\n",
    "    * $e_3 = (1,0)$, $s_3=(0,1)$\n",
    "    * $e_4 = (1,1)$, $s_4=(1,0)$\n",
    "    \n",
    "  corresponde a la compuerta XOR.\n",
    "\n",
    "2. Es el **perceptrón multicapa** capáz de aprender la compuerta XOR? Para responder esta pregunta, genere un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, $n_o=2$ neuronas ocultas y $n_s=2$ neuronas de salida, y entrénelo utilizando el conjunto de ejemplos de la compuerta XOR.\n",
    "\n",
    "3. Como se compara el **perceptrón multicapa** con el **perceptrón monocapa** sobre la compuerta XOR? Para responder esta otra pregunta, genere otro perceptrón \"multicapa\", pero esta vez utilizando solo dos capas, una de entrada con $n_e=2$ neuronas y una de salida con $n_s=2$ neuronas (de manera tal que en realidad es un perceptron monocapa), y repita el experimento anterior con los ejemplos de la compuerta XOR.\n",
    "\n",
    "4. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {
    "id": "UjbcNI0a4ac3"
   },
   "outputs": [],
   "source": [
    "# 1.1)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def puntos_aleatorios(n=3):\n",
    "    return [np.array([random.uniform(-10, 10), random.uniform(-10, 10)]) for _ in range(n)]\n",
    "\n",
    "num_puntos = [3,2,3]\n",
    "\n",
    "puntos = puntos_aleatorios()\n",
    "\n",
    "grupos = [[], [], []]\n",
    "\n",
    "for i in range(3):\n",
    "    for _ in range(num_puntos[i]):\n",
    "        grupos[i].append(puntos[i] + np.array(np.random.normal(0, 0.3, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "\n",
    "plt.scatter(*zip(*grupos[0]), color='purple', marker='o')\n",
    "plt.scatter(*zip(*grupos[1]), color='green', marker='o')\n",
    "plt.scatter(*zip(*grupos[2]), color='orange', marker='o')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 537,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.16943891, -0.12210515, -0.4886081 , -0.68732313, -1.32104289,\n",
       "        -2.30926718, -2.61820134, -3.15420476],\n",
       "       [-3.47897744, -2.74588753, -3.70718293,  0.59034762,  1.46159332,\n",
       "         7.36952846,  8.16605184,  8.19383841]])"
      ]
     },
     "execution_count": 537,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([coord for group in grupos for coord in group]).T\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 538,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labes(num_points):\n",
    "\n",
    "    filas = len(num_points)\n",
    "    columnas = sum(num_points)\n",
    "    \n",
    "    matriz = np.zeros((filas, columnas), dtype=int)\n",
    "    \n",
    "    col_inicio = 0\n",
    "    for i, num in enumerate(num_points):\n",
    "        matriz[i, col_inicio:col_inicio + num] = 1\n",
    "        col_inicio += num\n",
    "    \n",
    "    return matriz\n",
    "\n",
    "labels = generate_labes(num_points=num_puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 539,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights_gen(layers):\n",
    "    matrices = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        filas = layers[i+1] + 1\n",
    "        columnas = layers[i] + 1\n",
    "        matriz = np.random.rand(filas, columnas)\n",
    "        matrices.append(matriz)\n",
    "    return matrices\n",
    "\n",
    "data = np.array([coord for group in grupos for coord in group]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 540,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws_prueba = [np.array([[1, 1, 1],\n",
    "                       [1, 1, 1],\n",
    "                       [1, 1, 1]]),\n",
    "             np.array([[1, 1, 1, 1],\n",
    "                       [1, 1, 1, 1],])]\n",
    "\n",
    "layers_prueba = [2,3,2]\n",
    "entry_prueba = [1,2]\n",
    "\n",
    "# Ws_prueba = random_weights_gen(layers_prueba)\n",
    "\n",
    "def multi_layer_perceptron(entry, \n",
    "                           layers, \n",
    "                           Ws,\n",
    "                           act_fct = lambda x: x):\n",
    "\n",
    "    Ws_copy = Ws.copy()\n",
    "    layers_values = [np.array(entry).copy()]\n",
    "    hs = []\n",
    "    for k in range(len(layers) - 1):\n",
    "\n",
    "        layers_values[k] = np.append(layers_values[k], -1)\n",
    "\n",
    "        output = np.zeros(layers[k+1])\n",
    "        h = np.zeros(layers[k+1])\n",
    "        for j in range(layers[k+1]):\n",
    "            for i in range(layers[k] + 1):\n",
    "                output[j] += Ws_copy[k][j,i] * layers_values[k][i]\n",
    "            h[j] = output[j]\n",
    "            output[j] = act_fct(h[j])\n",
    "        hs.append(h)\n",
    "        layers_values.append(output.copy())\n",
    "\n",
    "    return layers_values, hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 541,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5.])"
      ]
     },
     "execution_count": 541,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = multi_layer_perceptron(entry=entry_prueba,\n",
    "                                layers=layers_prueba,\n",
    "                                Ws=Ws_prueba)[0]\n",
    "result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(Ws, \n",
    "                    layers,\n",
    "                    data, \n",
    "                    labels, \n",
    "                    learning_rate,\n",
    "                    act_fct=lambda x: x, \n",
    "                    d_act_fct=lambda x: 1):\n",
    "\n",
    "    Ws_copy = Ws.copy()\n",
    "\n",
    "    for m in range(labels.shape[1]): #labeled examples\n",
    "\n",
    "        layers_values, hs = multi_layer_perceptron(entry=data[:, m], \n",
    "                                                   layers=layers, \n",
    "                                                   Ws=Ws_copy, \n",
    "                                                   act_fct=act_fct)\n",
    "\n",
    "\n",
    "        D = np.zeros(len(layers_values[-1]))\n",
    "        for i in range(len(layers_values[-1])):\n",
    "            D[i] += (layers_values[-1][i] - labels[i, m]) * d_act_fct(hs[-1][i])\n",
    "        \n",
    "        D_ls = [D.copy()]\n",
    "        for i in range(1, len(layers_values) - 1):\n",
    "            D = np.zeros(len(layers_values[-i-1]))\n",
    "            for j in range(len(layers_values[-i-1]) - 1):\n",
    "\n",
    "                D[j] += d_act_fct(hs[-i-1][j]) * D_ls[-i] @ Ws[-i][:,-i]\n",
    "            \n",
    "            D_ls.append(D.copy())\n",
    "        \n",
    "        D_ls = D_ls[::-1]\n",
    "\n",
    "        print(len(layers_values))\n",
    "        print(len(D_ls))\n",
    "        print(len(Ws_copy))\n",
    "        for l in range(1,len(layers_values)):\n",
    "            for i in range(len(layers_values[l])):\n",
    "                for j in range(len(layers_values[l+1])-1):\n",
    "                    Ws_copy[l-1][j,i] -= learning_rate * D_ls[l-1][j] * layers_values[l][i] ##Creo que aca esta el error, en los indices\n",
    "\n",
    "    return Ws_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 543,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3\n",
      "2\n",
      "2\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 1 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[543], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWs_prueba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayers_prueba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[542], line 40\u001b[0m, in \u001b[0;36mbackpropagation\u001b[1;34m(Ws, layers, data, labels, learning_rate, act_fct, d_act_fct)\u001b[0m\n\u001b[0;32m     38\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers_values[l])):\n\u001b[0;32m     39\u001b[0m             \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers_values[l\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 40\u001b[0m                 \u001b[43mWs_copy\u001b[49m\u001b[43m[\u001b[49m\u001b[43ml\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m,\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m learning_rate \u001b[38;5;241m*\u001b[39m D_ls[l][j] \u001b[38;5;241m*\u001b[39m layers_values[l][i]\n\u001b[0;32m     42\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Ws_copy\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 1 with size 3"
     ]
    }
   ],
   "source": [
    "W = backpropagation(Ws=Ws_prueba,\n",
    "                    data=data,\n",
    "                    layers=layers_prueba,\n",
    "                    labels=labels,\n",
    "                    learning_rate=0.02)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
