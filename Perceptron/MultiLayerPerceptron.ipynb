{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRYEofSD0xoF"
   },
   "source": [
    "# Perceptrón multicapa\n",
    "\n",
    "Consideraremos un perceptrón multicapa, con capas enumeradas por $l=0,1,...,L$.\n",
    "Denotemos por $x^l_i$ el estado de la $i$-ésima neurona en la capa $l$.\n",
    "Diremos que la red posee $n^l$ neuronas $i=1,...,n^l$ en la $l$-ésima capa.\n",
    "En particular, $x^0$ denota el vector de estados de la capa de entrada y $x^L$ el vector de estados de la capa de salida.\n",
    "Se tiene que\n",
    "\\begin{equation}\n",
    "x^l_i\n",
    "=\n",
    "g(h^l_i)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (1)\n",
    "\\end{equation}\n",
    "donde $g:\\mathbb{R}\\to \\mathbb{R}$ es una función de activación, por ejemplo una sigmoide $g(h)=1/(1+e^{-h})$, y\n",
    "\\begin{equation}\n",
    "h^{l}_i\n",
    "=\n",
    "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (2)\n",
    "\\end{equation}\n",
    "es el campo local sufrido por la $i$-ésima neurona en la $l$-ésima capa .\n",
    "Además, $w^l_{ij}$ denota la intensidad de la sinapsis que conecta la $j$-ésima neurona en la $(l-1)$-ésima capa con la $i$-ésima neurona en la $l$-ésima capa.\n",
    "Notar, la red depende de las matrices de pesos sinápticos $w^1,w^2,...,w^{L}$.\n",
    "\n",
    "## Umbrales de activación\n",
    "\n",
    "En cada una de las capas $l=0,1,...,L-1$, se agrega una neurona extra $i=n^l+1$ con un estado fijo $x^l_{n^l+1}=-1$.\n",
    "De esta manera, una nueva sinapsis $u^{l}_i:=w^{l}_{i,n^{l-1}+1}$ hace las veces de umbral de activación de la $i$-ésima neurona en la $l$-ésima capa, ya que\n",
    "\\begin{equation}\n",
    "h^{l+1}_i\n",
    "=\n",
    "w^{l+1}_{i,n^{l}+1} x^{l}_{n^{l}+1}\n",
    "+\n",
    "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^{l}_j\n",
    "=\n",
    "-\n",
    "u^{l+1}_i\n",
    "+\n",
    "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^l_j\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (3)\n",
    "\\end{equation}\n",
    "\n",
    "## Conjunto de entrenamiento\n",
    "\n",
    "Los datos de entrenamiento consisten en un conjunto de pares $\\{(e^m,s^m):m=1,...,M\\}$ donde $e^m\\in \\mathbb{R}^{n_0}$ y $s^m\\in \\mathbb{R}^{n_L}$ son vectores que representan el $m$-ésimo par de entrada-salida o *ejemplo* que debe aprender la red.\n",
    "\n",
    "## Función costo: el Error Cuadrático\n",
    "\n",
    "Si pensamos que la salida de la red es una función de la entrada, i.e. que $x^L(x^0)$, podemos evaluar el error que comete la red sobre el conjunto de entramiento utilizando el *error cuadrático*\n",
    "$$\n",
    "E\n",
    "=\n",
    "\\sum_{m=1}^M F^m\n",
    "$$\n",
    "como *función costo*, donde\n",
    "$$\n",
    "F^m\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\sum_{i=1}^{n^L}\n",
    "(x^L_i(x^0=e^m) - s^m_i)^2\n",
    "$$\n",
    "es el error cuadrático que comete la red sobre el $m$-ésimo ejemplo.\n",
    "\n",
    "## Entrenamiento: descenso por el gradiente\n",
    "\n",
    "Entrenar la red consisten en encontrar valores de los pesos sinápticos $w^l_{ij}$ que minimicen el error $E$.\n",
    "Para ello, expresamos el error en función de dichos pesos y calculamos las componentes de su gradiente\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^l_{ij}}\n",
    "=\n",
    "\\sum_m\n",
    "\\frac{\\partial F^m}{\\partial w^l_{ij}}\n",
    "$$\n",
    "De esta manera, podemos utilizar el algoritmo de descenso por el gradiente para actualizar los pesos hasta que el error alcance un mínimo global.\n",
    "Más precisamente, partiendo de valores aleatorios\n",
    "$(w^l_{ij})^0$ para los pesos sinápticos, actualizamos iterativamente a los mismos con la siguiente regla\n",
    "\\begin{equation}\n",
    "(w^l_{ij})^{t+1} = (w^l_{ij})^t-\\eta \\frac{\\partial F^m}{\\partial w^l_{ij}}((w^l_{ij})^t)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (4)\n",
    "\\end{equation}\n",
    "para todo $l$, $ij$ y $m$, donde el parámetro $0<\\eta\\ll 1$ controla la tasa de aprendizaje.\n",
    "La iteración se detiene cuando ya no se advierten reducciones significativas del error $E$.\n",
    "\n",
    "## Cálculo del gradiente del error cuadrático\n",
    "\n",
    "Con el fin de simplificar la notación, elegimos un valor arbitrario de $m$ y obviamos la dependencia de las expresiones con éste índice.\n",
    "\n",
    "Notar que los vectores $x^l$ y $h^l$ sólo dependen de las matrices $w^1,...,w^{l}$.\n",
    "De esta manera, observamos que\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "g'(h^l_i)\n",
    "\\frac{\\partial h^l_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "si $r\\leq l$, y\n",
    "$$\n",
    "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}=0\n",
    "$$\n",
    "en caso contrario.\n",
    "Por otro lado,\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial h^{l}_i}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\frac{\\partial}{\\partial w^r_{pq}}\n",
    "\\bigg(\n",
    "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
    "\\bigg)\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j w^{l}_{ij}\n",
    "\\frac{\\partial x^{l-1}_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "si $r<l$, y\n",
    "$$\n",
    "\\frac{\\partial h^l_i}{\\partial w^{l}_{pq}}\n",
    "=\n",
    "\\sum_j\n",
    "\\delta_{ip}\n",
    "\\delta_{jq}\n",
    "x^{l-1}_j\n",
    "=\n",
    "\\delta_{ip}\n",
    "x^{l-1}_q\n",
    "$$\n",
    "Con estas ecuaciones se pueden establecer una relación de recurrencia que nos permite calcular las componentes del gradiente de $F$.\n",
    "A saber\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_i (x^L_i-s_i)\n",
    "\\frac{\\partial x^L}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i (x^L_i-s_i)\n",
    "g'(h^L_i)\n",
    "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\sum_j\n",
    "w^L_{ij}\n",
    "\\frac{\\partial x^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\sum_j\n",
    "w^L_{ij}\n",
    "g'(h^{L-1}_j)\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "\\bigg(\n",
    "g'(h^{L-1}_j)\n",
    "\\sum_i\n",
    "w^L_{ij}\n",
    "D^L_i\n",
    "\\bigg)\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "D^{L-1}_j\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "donde\n",
    "\\begin{equation}\n",
    "D^L_i:=(x^L_i-s_i)g'(h^L_i)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (5)\n",
    "\\end{equation}\n",
    "y\n",
    "$$\n",
    "D^{L-1}_j\n",
    ":=\n",
    "g'(h^{L-1}_j)\n",
    "\\sum_i\n",
    "w^L_{ij}\n",
    "D^L_i\n",
    "$$\n",
    "representan los *errores locales* de la $i$-ésima neurona en la $L$-ésima capa y la $j$-ésima neurona en la $(L-1)$-ésima capa, respectivamente.\n",
    "\n",
    "El anterior procedimiento puede continuarse capa por capa, con cada capa $l$ tal que $r<l$, de manera que\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_j D_j^l\n",
    "\\frac{\\partial h^l_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "donde\n",
    "\\begin{equation}\n",
    "D_j^l\n",
    ":=\n",
    "g'(h^{l}_j)\n",
    "\\sum_i w^{l+1}_{ij}D_i^{l+1}\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (6)\n",
    "\\end{equation}\n",
    "hasta que eventualmente se alcanza la capa $l=r$, y se obtiene\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_j\n",
    "D_j^{r}\n",
    "\\frac{\\partial h^{r}_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "D_j^{r}\n",
    "\\delta_{jp}\n",
    "x^{r-1}_q\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "D_p^{r}\n",
    "x^{r-1}_q\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "En particular, este último resultado se verifica para el caso $r=L$ de $pq$ arbitrario.\n",
    "También se verifica para el caso en que $q=n^{r-1}+1$ y valores arbitrarios de $r$ y $p$, en donde $x_q^{r-1}=-1$ corresponde al estado fijo de la neurona en la capa $(r-1)$-ésima que permite simular la acción de umbrales en la capa $r$-ésima, tal como se describe en la Ec. 3.\n",
    "\n",
    "## El algoritmo de backpropagation\n",
    "\n",
    "Los resultados anteriores pueden condensarse en el llamado *algoritmo de backpropagation*, el cuál permite el cálculo del gradiente y la actualización de los pesos sinápticos, y consiste en la siguiente lista de pasos.\n",
    "Para cada ejemplo $m=1,...,M$, ejecutar:\n",
    "1. *Forward pass:* calcular la salida $x^L$ de la red ante la entrada $x^1=e^m$ utilizando las Ecs. 1 y 2. En el proceso, guardar los valores de activación $x^l$ y de los correspondientes campos locales $h^l$ obtenidos en las distintas capas $l=2,...,L$, ya que serán útiles más adelante.\n",
    "2. Calcular el vector de errores $D^L$ de la capa de salida utilizando la Ec. 5.\n",
    "3. Propagar los errores hacia atrás, i.e. calcular los errores $D^l$ para $l=L-1,L-2,...,1$ utilizando la Ec. 6.\n",
    "4. Para cada $l$, $i$ y $j$, calcular el gradiente $\\frac{\\partial F^m}{\\partial w^l_{ij}}$ utilizando la Ec. 7 y actualizar el correspondiente peso sináptico $w^l_{ij}$ utilizando la Ec. 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XeWtDvx6C8A"
   },
   "source": [
    "## **Ejercicio 1**\n",
    "\n",
    "Genere un conjunto de entrenamiento compuesto por $M=\\sum_c m_c$ puntos en $\\mathbb{R}^{n_e}$ distribuidos en $n_s$ nubes de $m_c$ puntos.\n",
    "\n",
    "Para generar las nubes:\n",
    "\n",
    "* genere aleatoriamente $n_s$ puntos en $\\mathbb{R}^{n_e}$ a los que llamaremos centros, sorteando los valores de las coordenadas a partir de una distribución normal, y\n",
    "\n",
    "* para cada centro $c$, genere $m_c$ puntos aleatorios alrededor del mismo, sumando sus coordenadas a números aleatorios generados con una Gaussiana de varianza $\\sigma^2$.\n",
    "\n",
    "Las $n_e$ coordenadas del $m$-ésimo punto constituirán el vector de entrada del $m$-ésimo ejemplo.\n",
    "La nube a la que pertenece el $m$-ésimo punto determinará el vector de salida del $m$-ésimo ejemplo.\n",
    "Más precisamente, si el $m$-ésimo punto pertenence a la $c$-ésima nube, el vector de salida será el vector canónico $(0,0,...,1,...,0)$ de $n_s$ componentes con un único 1 en la $c$-esima posición.\n",
    "\n",
    "Concretamente\n",
    "\n",
    "1. Genere un conjunto de 8 puntos en $\\mathbb{R}^{n_e}$ con $n_e=2$, divididos en 3 nubes con $m_1=3$ en la primera nube, $m_2=2$ puntos en la segunda nube y $m_3=3$ puntos en la tercera nube. Utilice $\\sigma=0.1$ para indicar la dispersión de los puntos alrededor de cada nube.\n",
    "\n",
    "2. Grafique las nubes de puntos, utilizando un color distinto para cada una de ellas.\n",
    "\n",
    "## **Ejercicio 2**\n",
    "\n",
    "1. Implemente un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, una capa oculta de $n_o=2$ neuronas, y una capa de salida de $n_s=3$ neuronas. Recuerde, además, agregar las neuroas auxiliares que se utiliza para imitar los umbrales de activación. Utilice funciones de activación **sigmoideas**.\n",
    "\n",
    "2. Entrenelo sobre el conjunto de ejemplos generado en el Ejercicio 1. Para entrenarlo, utilice una tasa $\\eta=0.02$ y alrededor de 10.000 de épocas o más, según considere necesario.\n",
    "\n",
    "3. Grafique el error $E$ en función del número de épocas de entrenamiento.\n",
    "\n",
    "4. Luego, grafique nuevamente los puntos del Ejercicio 1, pintando el relleno de los mismos con los colores correspondiente a cada nube, y el borde de los mismos con el color correspondiente a la predicción obtenida con el **perceptrón multicapa**. Coinciden las predicciones con los colores originales?\n",
    "\n",
    "5. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?\n",
    "\n",
    "## **Ejercicio 3: la compuerta XOR**\n",
    "\n",
    "1. Fabrique un dataset con el siguiente conjunto de 4 ejemplos:\n",
    "\n",
    "    * $e_1 = (0,0)$, $s_1=(1,0)$\n",
    "    * $e_2 = (0,1)$, $s_2=(0,1)$\n",
    "    * $e_3 = (1,0)$, $s_3=(0,1)$\n",
    "    * $e_4 = (1,1)$, $s_4=(1,0)$\n",
    "    \n",
    "  corresponde a la compuerta XOR.\n",
    "\n",
    "2. Es el **perceptrón multicapa** capáz de aprender la compuerta XOR? Para responder esta pregunta, genere un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, $n_o=2$ neuronas ocultas y $n_s=2$ neuronas de salida, y entrénelo utilizando el conjunto de ejemplos de la compuerta XOR.\n",
    "\n",
    "3. Como se compara el **perceptrón multicapa** con el **perceptrón monocapa** sobre la compuerta XOR? Para responder esta otra pregunta, genere otro perceptrón \"multicapa\", pero esta vez utilizando solo dos capas, una de entrada con $n_e=2$ neuronas y una de salida con $n_s=2$ neuronas (de manera tal que en realidad es un perceptron monocapa), y repita el experimento anterior con los ejemplos de la compuerta XOR.\n",
    "\n",
    "4. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {
    "id": "UjbcNI0a4ac3"
   },
   "outputs": [],
   "source": [
    "# 1.1)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def puntos_aleatorios(n=3):\n",
    "    return [np.array([random.uniform(-10, 10), random.uniform(-10, 10)]) for _ in range(n)]\n",
    "\n",
    "num_puntos = [10,8,10]\n",
    "\n",
    "puntos = puntos_aleatorios()\n",
    "\n",
    "grupos = [[], [], []]\n",
    "\n",
    "for i in range(3):\n",
    "    for _ in range(num_puntos[i]):\n",
    "        grupos[i].append(puntos[i] + np.array(np.random.normal(0, 0.3, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "\n",
    "plt.scatter(*zip(*grupos[0]), color='purple', marker='o')\n",
    "plt.scatter(*zip(*grupos[1]), color='green', marker='o')\n",
    "plt.scatter(*zip(*grupos[2]), color='orange', marker='o')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 5.15479014,  4.60964124,  4.54610446,  4.8079947 ,  5.08108173,\n",
       "         4.96140023,  5.03457957,  5.02487461,  5.56367196,  4.48548886,\n",
       "        -2.33445435, -2.28902619, -2.56160635, -2.50355454, -2.57089815,\n",
       "        -2.44098196, -2.36395877, -2.64755723,  7.70089355,  8.47459864,\n",
       "         8.17125415,  8.27399997,  8.18023201,  8.4002976 ,  8.52594401,\n",
       "         8.37149003,  8.29537195,  7.90041155],\n",
       "       [-9.57004411, -8.53009854, -8.44014247, -8.93727057, -8.61344442,\n",
       "        -8.95523545, -8.71409936, -9.3011923 , -8.46107851, -8.8887982 ,\n",
       "         4.72570967,  4.32184841,  4.29421298,  4.3892017 ,  4.44213709,\n",
       "         4.78968317,  4.80806768,  5.34958225, -5.54486053, -5.87519645,\n",
       "        -6.33011189, -6.08909928, -5.99650553, -5.94381852, -5.59664829,\n",
       "        -5.56706359, -5.97586196, -6.1436391 ]])"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([coord for group in grupos for coord in group]).T\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labes(num_points):\n",
    "\n",
    "    filas = len(num_points)\n",
    "    columnas = sum(num_points)\n",
    "    \n",
    "    matriz = np.zeros((filas, columnas), dtype=int)\n",
    "    \n",
    "    col_inicio = 0\n",
    "    for i, num in enumerate(num_points):\n",
    "        matriz[i, col_inicio:col_inicio + num] = 1\n",
    "        col_inicio += num\n",
    "    \n",
    "    return matriz\n",
    "\n",
    "labels = generate_labes(num_points=num_puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights_gen(layers):\n",
    "    matrices = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        filas = layers[i+1] + 1\n",
    "        columnas = layers[i] + 1\n",
    "        matriz = np.random.rand(filas, columnas)\n",
    "        matrices.append(matriz)\n",
    "    return matrices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws_prueba = [np.array([[1, 1, 1],\n",
    "                       [1, 1, 1],\n",
    "                       [1, 1, 1],\n",
    "                       [1, 1, 1]]),\n",
    "             np.array([[1, 1, 1, 1],\n",
    "                       [1, 1, 1, 1]]),\n",
    "             np.array([[1,1,1],\n",
    "                       [1,1,1],\n",
    "                       [1,1,1],\n",
    "                       [1,1,1],\n",
    "                       [1,1,1]])]\n",
    "\n",
    "layers_prueba = [2,3,2,5]\n",
    "entry_prueba = [1,2]\n",
    "\n",
    "# Ws_prueba = random_weights_gen(layers_prueba)\n",
    "\n",
    "def multi_layer_perceptron(entry, \n",
    "                           layers, \n",
    "                           Ws,\n",
    "                           act_fct = lambda x: x):\n",
    "\n",
    "    Ws_copy = Ws.copy()\n",
    "    entry_copy = np.array(entry).copy()\n",
    "    for k in range(len(layers) - 1):\n",
    "\n",
    "        entry_copy = np.append(entry_copy, -1)\n",
    "\n",
    "        output = np.zeros(layers[k+1])\n",
    "        for j in range(layers[k+1]):\n",
    "            for i in range(layers[k] + 1):\n",
    "                output[j] += Ws_copy[k][j,i] * entry_copy[i]\n",
    "            output[j] = act_fct(output[j])\n",
    "        \n",
    "        entry_copy = output.copy()\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([9., 9., 9., 9., 9.])"
      ]
     },
     "execution_count": 210,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = multi_layer_perceptron(entry=entry_prueba,\n",
    "                                layers=layers_prueba,\n",
    "                                Ws=Ws_prueba)\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
