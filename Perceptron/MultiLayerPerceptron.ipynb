{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NRYEofSD0xoF"
   },
   "source": [
    "# Perceptrón multicapa\n",
    "\n",
    "Consideraremos un perceptrón multicapa, con capas enumeradas por $l=0,1,...,L$.\n",
    "Denotemos por $x^l_i$ el estado de la $i$-ésima neurona en la capa $l$.\n",
    "Diremos que la red posee $n^l$ neuronas $i=1,...,n^l$ en la $l$-ésima capa.\n",
    "En particular, $x^0$ denota el vector de estados de la capa de entrada y $x^L$ el vector de estados de la capa de salida.\n",
    "Se tiene que\n",
    "\\begin{equation}\n",
    "x^l_i\n",
    "=\n",
    "g(h^l_i)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (1)\n",
    "\\end{equation}\n",
    "donde $g:\\mathbb{R}\\to \\mathbb{R}$ es una función de activación, por ejemplo una sigmoide $g(h)=1/(1+e^{-h})$, y\n",
    "\\begin{equation}\n",
    "h^{l}_i\n",
    "=\n",
    "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (2)\n",
    "\\end{equation}\n",
    "es el campo local sufrido por la $i$-ésima neurona en la $l$-ésima capa .\n",
    "Además, $w^l_{ij}$ denota la intensidad de la sinapsis que conecta la $j$-ésima neurona en la $(l-1)$-ésima capa con la $i$-ésima neurona en la $l$-ésima capa.\n",
    "Notar, la red depende de las matrices de pesos sinápticos $w^1,w^2,...,w^{L}$.\n",
    "\n",
    "## Umbrales de activación\n",
    "\n",
    "En cada una de las capas $l=0,1,...,L-1$, se agrega una neurona extra $i=n^l+1$ con un estado fijo $x^l_{n^l+1}=-1$.\n",
    "De esta manera, una nueva sinapsis $u^{l}_i:=w^{l}_{i,n^{l-1}+1}$ hace las veces de umbral de activación de la $i$-ésima neurona en la $l$-ésima capa, ya que\n",
    "\\begin{equation}\n",
    "h^{l+1}_i\n",
    "=\n",
    "w^{l+1}_{i,n^{l}+1} x^{l}_{n^{l}+1}\n",
    "+\n",
    "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^{l}_j\n",
    "=\n",
    "-\n",
    "u^{l+1}_i\n",
    "+\n",
    "\\sum_{j=1}^{n^l} w^{l+1}_{ij} x^l_j\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (3)\n",
    "\\end{equation}\n",
    "\n",
    "## Conjunto de entrenamiento\n",
    "\n",
    "Los datos de entrenamiento consisten en un conjunto de pares $\\{(e^m,s^m):m=1,...,M\\}$ donde $e^m\\in \\mathbb{R}^{n_0}$ y $s^m\\in \\mathbb{R}^{n_L}$ son vectores que representan el $m$-ésimo par de entrada-salida o *ejemplo* que debe aprender la red.\n",
    "\n",
    "## Función costo: el Error Cuadrático\n",
    "\n",
    "Si pensamos que la salida de la red es una función de la entrada, i.e. que $x^L(x^0)$, podemos evaluar el error que comete la red sobre el conjunto de entramiento utilizando el *error cuadrático*\n",
    "$$\n",
    "E\n",
    "=\n",
    "\\sum_{m=1}^M F^m\n",
    "$$\n",
    "como *función costo*, donde\n",
    "$$\n",
    "F^m\n",
    "=\n",
    "\\frac{1}{2}\n",
    "\\sum_{i=1}^{n^L}\n",
    "(x^L_i(x^0=e^m) - s^m_i)^2\n",
    "$$\n",
    "es el error cuadrático que comete la red sobre el $m$-ésimo ejemplo.\n",
    "\n",
    "## Entrenamiento: descenso por el gradiente\n",
    "\n",
    "Entrenar la red consisten en encontrar valores de los pesos sinápticos $w^l_{ij}$ que minimicen el error $E$.\n",
    "Para ello, expresamos el error en función de dichos pesos y calculamos las componentes de su gradiente\n",
    "$$\n",
    "\\frac{\\partial E}{\\partial w^l_{ij}}\n",
    "=\n",
    "\\sum_m\n",
    "\\frac{\\partial F^m}{\\partial w^l_{ij}}\n",
    "$$\n",
    "De esta manera, podemos utilizar el algoritmo de descenso por el gradiente para actualizar los pesos hasta que el error alcance un mínimo global.\n",
    "Más precisamente, partiendo de valores aleatorios\n",
    "$(w^l_{ij})^0$ para los pesos sinápticos, actualizamos iterativamente a los mismos con la siguiente regla\n",
    "\\begin{equation}\n",
    "(w^l_{ij})^{t+1} = (w^l_{ij})^t-\\eta \\frac{\\partial F^m}{\\partial w^l_{ij}}((w^l_{ij})^t)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (4)\n",
    "\\end{equation}\n",
    "para todo $l$, $ij$ y $m$, donde el parámetro $0<\\eta\\ll 1$ controla la tasa de aprendizaje.\n",
    "La iteración se detiene cuando ya no se advierten reducciones significativas del error $E$.\n",
    "\n",
    "## Cálculo del gradiente del error cuadrático\n",
    "\n",
    "Con el fin de simplificar la notación, elegimos un valor arbitrario de $m$ y obviamos la dependencia de las expresiones con éste índice.\n",
    "\n",
    "Notar que los vectores $x^l$ y $h^l$ sólo dependen de las matrices $w^1,...,w^{l}$.\n",
    "De esta manera, observamos que\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "g'(h^l_i)\n",
    "\\frac{\\partial h^l_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "si $r\\leq l$, y\n",
    "$$\n",
    "\\frac{\\partial x^l_i}{\\partial w^r_{pq}}=0\n",
    "$$\n",
    "en caso contrario.\n",
    "Por otro lado,\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial h^{l}_i}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\frac{\\partial}{\\partial w^r_{pq}}\n",
    "\\bigg(\n",
    "\\sum_j w^{l}_{ij} x^{l-1}_j\n",
    "\\bigg)\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j w^{l}_{ij}\n",
    "\\frac{\\partial x^{l-1}_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "si $r<l$, y\n",
    "$$\n",
    "\\frac{\\partial h^l_i}{\\partial w^{l}_{pq}}\n",
    "=\n",
    "\\sum_j\n",
    "\\delta_{ip}\n",
    "\\delta_{jq}\n",
    "x^{l-1}_j\n",
    "=\n",
    "\\delta_{ip}\n",
    "x^{l-1}_q\n",
    "$$\n",
    "Con estas ecuaciones se pueden establecer una relación de recurrencia que nos permite calcular las componentes del gradiente de $F$.\n",
    "A saber\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_i (x^L_i-s_i)\n",
    "\\frac{\\partial x^L}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i (x^L_i-s_i)\n",
    "g'(h^L_i)\n",
    "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\frac{\\partial h^L_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\sum_j\n",
    "w^L_{ij}\n",
    "\\frac{\\partial x^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_i\n",
    "D^L_i\n",
    "\\sum_j\n",
    "w^L_{ij}\n",
    "g'(h^{L-1}_j)\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "\\bigg(\n",
    "g'(h^{L-1}_j)\n",
    "\\sum_i\n",
    "w^L_{ij}\n",
    "D^L_i\n",
    "\\bigg)\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "D^{L-1}_j\n",
    "\\frac{\\partial h^{L-1}_i}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "donde\n",
    "\\begin{equation}\n",
    "D^L_i:=(x^L_i-s_i)g'(h^L_i)\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (5)\n",
    "\\end{equation}\n",
    "y\n",
    "$$\n",
    "D^{L-1}_j\n",
    ":=\n",
    "g'(h^{L-1}_j)\n",
    "\\sum_i\n",
    "w^L_{ij}\n",
    "D^L_i\n",
    "$$\n",
    "representan los *errores locales* de la $i$-ésima neurona en la $L$-ésima capa y la $j$-ésima neurona en la $(L-1)$-ésima capa, respectivamente.\n",
    "\n",
    "El anterior procedimiento puede continuarse capa por capa, con cada capa $l$ tal que $r<l$, de manera que\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_j D_j^l\n",
    "\\frac{\\partial h^l_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "donde\n",
    "\\begin{equation}\n",
    "D_j^l\n",
    ":=\n",
    "g'(h^{l}_j)\n",
    "\\sum_i w^{l+1}_{ij}D_i^{l+1}\n",
    "\\;\\;\\;\\;\\;\\;\\;\\; (6)\n",
    "\\end{equation}\n",
    "hasta que eventualmente se alcanza la capa $l=r$, y se obtiene\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial F}{\\partial w^r_{pq}}\n",
    "&=&\n",
    "\\sum_j\n",
    "D_j^{r}\n",
    "\\frac{\\partial h^{r}_j}{\\partial w^r_{pq}}\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "\\sum_j\n",
    "D_j^{r}\n",
    "\\delta_{jp}\n",
    "x^{r-1}_q\n",
    "\\nonumber\n",
    "\\\\\n",
    "&=&\n",
    "D_p^{r}\n",
    "x^{r-1}_q\n",
    "\\nonumber\n",
    "\\end{eqnarray}\n",
    "En particular, este último resultado se verifica para el caso $r=L$ de $pq$ arbitrario.\n",
    "También se verifica para el caso en que $q=n^{r-1}+1$ y valores arbitrarios de $r$ y $p$, en donde $x_q^{r-1}=-1$ corresponde al estado fijo de la neurona en la capa $(r-1)$-ésima que permite simular la acción de umbrales en la capa $r$-ésima, tal como se describe en la Ec. 3.\n",
    "\n",
    "## El algoritmo de backpropagation\n",
    "\n",
    "Los resultados anteriores pueden condensarse en el llamado *algoritmo de backpropagation*, el cuál permite el cálculo del gradiente y la actualización de los pesos sinápticos, y consiste en la siguiente lista de pasos.\n",
    "Para cada ejemplo $m=1,...,M$, ejecutar:\n",
    "1. *Forward pass:* calcular la salida $x^L$ de la red ante la entrada $x^1=e^m$ utilizando las Ecs. 1 y 2. En el proceso, guardar los valores de activación $x^l$ y de los correspondientes campos locales $h^l$ obtenidos en las distintas capas $l=2,...,L$, ya que serán útiles más adelante.\n",
    "2. Calcular el vector de errores $D^L$ de la capa de salida utilizando la Ec. 5.\n",
    "3. Propagar los errores hacia atrás, i.e. calcular los errores $D^l$ para $l=L-1,L-2,...,1$ utilizando la Ec. 6.\n",
    "4. Para cada $l$, $i$ y $j$, calcular el gradiente $\\frac{\\partial F^m}{\\partial w^l_{ij}}$ utilizando la Ec. 7 y actualizar el correspondiente peso sináptico $w^l_{ij}$ utilizando la Ec. 4."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-XeWtDvx6C8A"
   },
   "source": [
    "## **Ejercicio 1**\n",
    "\n",
    "Genere un conjunto de entrenamiento compuesto por $M=\\sum_c m_c$ puntos en $\\mathbb{R}^{n_e}$ distribuidos en $n_s$ nubes de $m_c$ puntos.\n",
    "\n",
    "Para generar las nubes:\n",
    "\n",
    "* genere aleatoriamente $n_s$ puntos en $\\mathbb{R}^{n_e}$ a los que llamaremos centros, sorteando los valores de las coordenadas a partir de una distribución normal, y\n",
    "\n",
    "* para cada centro $c$, genere $m_c$ puntos aleatorios alrededor del mismo, sumando sus coordenadas a números aleatorios generados con una Gaussiana de varianza $\\sigma^2$.\n",
    "\n",
    "Las $n_e$ coordenadas del $m$-ésimo punto constituirán el vector de entrada del $m$-ésimo ejemplo.\n",
    "La nube a la que pertenece el $m$-ésimo punto determinará el vector de salida del $m$-ésimo ejemplo.\n",
    "Más precisamente, si el $m$-ésimo punto pertenence a la $c$-ésima nube, el vector de salida será el vector canónico $(0,0,...,1,...,0)$ de $n_s$ componentes con un único 1 en la $c$-esima posición.\n",
    "\n",
    "Concretamente\n",
    "\n",
    "1. Genere un conjunto de 8 puntos en $\\mathbb{R}^{n_e}$ con $n_e=2$, divididos en 3 nubes con $m_1=3$ en la primera nube, $m_2=2$ puntos en la segunda nube y $m_3=3$ puntos en la tercera nube. Utilice $\\sigma=0.1$ para indicar la dispersión de los puntos alrededor de cada nube.\n",
    "\n",
    "2. Grafique las nubes de puntos, utilizando un color distinto para cada una de ellas.\n",
    "\n",
    "## **Ejercicio 2**\n",
    "\n",
    "1. Implemente un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, una capa oculta de $n_o=2$ neuronas, y una capa de salida de $n_s=3$ neuronas. Recuerde, además, agregar las neuroas auxiliares que se utiliza para imitar los umbrales de activación. Utilice funciones de activación **sigmoideas**.\n",
    "\n",
    "2. Entrenelo sobre el conjunto de ejemplos generado en el Ejercicio 1. Para entrenarlo, utilice una tasa $\\eta=0.02$ y alrededor de 10.000 de épocas o más, según considere necesario.\n",
    "\n",
    "3. Grafique el error $E$ en función del número de épocas de entrenamiento.\n",
    "\n",
    "4. Luego, grafique nuevamente los puntos del Ejercicio 1, pintando el relleno de los mismos con los colores correspondiente a cada nube, y el borde de los mismos con el color correspondiente a la predicción obtenida con el **perceptrón multicapa**. Coinciden las predicciones con los colores originales?\n",
    "\n",
    "5. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?\n",
    "\n",
    "## **Ejercicio 3: la compuerta XOR**\n",
    "\n",
    "1. Fabrique un dataset con el siguiente conjunto de 4 ejemplos:\n",
    "\n",
    "    * $e_1 = (0,0)$, $s_1=(1,0)$\n",
    "    * $e_2 = (0,1)$, $s_2=(0,1)$\n",
    "    * $e_3 = (1,0)$, $s_3=(0,1)$\n",
    "    * $e_4 = (1,1)$, $s_4=(1,0)$\n",
    "    \n",
    "  corresponde a la compuerta XOR.\n",
    "\n",
    "2. Es el **perceptrón multicapa** capáz de aprender la compuerta XOR? Para responder esta pregunta, genere un **perceptrón multicapa** con $n_e=2$ neuronas de entrada, $n_o=2$ neuronas ocultas y $n_s=2$ neuronas de salida, y entrénelo utilizando el conjunto de ejemplos de la compuerta XOR.\n",
    "\n",
    "3. Como se compara el **perceptrón multicapa** con el **perceptrón monocapa** sobre la compuerta XOR? Para responder esta otra pregunta, genere otro perceptrón \"multicapa\", pero esta vez utilizando solo dos capas, una de entrada con $n_e=2$ neuronas y una de salida con $n_s=2$ neuronas (de manera tal que en realidad es un perceptron monocapa), y repita el experimento anterior con los ejemplos de la compuerta XOR.\n",
    "\n",
    "4. Repita los experimentos con funciones de activación **ReLUs**. Que ocurre?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "id": "UjbcNI0a4ac3"
   },
   "outputs": [],
   "source": [
    "# 1.1)\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "\n",
    "def puntos_aleatorios(n=3):\n",
    "    return [np.array([random.uniform(-10, 10), random.uniform(-10, 10)]) for _ in range(n)]\n",
    "\n",
    "num_puntos = [3,2,3]\n",
    "\n",
    "puntos = puntos_aleatorios()\n",
    "\n",
    "grupos = [[], [], []]\n",
    "\n",
    "for i in range(3):\n",
    "    for _ in range(num_puntos[i]):\n",
    "        grupos[i].append(puntos[i] + np.array(np.random.normal(0, 0.3, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1200x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1, figsize=(12,6))\n",
    "\n",
    "plt.scatter(*zip(*grupos[0]), color='purple', marker='o')\n",
    "plt.scatter(*zip(*grupos[1]), color='green', marker='o')\n",
    "plt.scatter(*zip(*grupos[2]), color='orange', marker='o')\n",
    "\n",
    "plt.grid()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.87222827, -2.11567937, -2.11020058, -9.15035342, -9.88446445,\n",
       "        -1.88674157, -2.19885466, -1.98174739],\n",
       "       [ 7.20415394,  7.02323318,  7.10984755,  2.29630822,  2.55192204,\n",
       "        -5.25613612, -5.21275867, -5.6169982 ]])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([coord for group in grupos for coord in group]).T\n",
    "\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_labes(num_points):\n",
    "\n",
    "    filas = len(num_points)\n",
    "    columnas = sum(num_points)\n",
    "    \n",
    "    matriz = np.zeros((filas, columnas), dtype=int)\n",
    "    \n",
    "    col_inicio = 0\n",
    "    for i, num in enumerate(num_points):\n",
    "        matriz[i, col_inicio:col_inicio + num] = 1\n",
    "        col_inicio += num\n",
    "    \n",
    "    return matriz\n",
    "\n",
    "labels = generate_labes(num_points=num_puntos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_weights_gen(layers):\n",
    "    matrices = []\n",
    "    for i in range(len(layers) - 1):\n",
    "        filas = layers[i+1] + 1\n",
    "        columnas = layers[i] + 1\n",
    "        matriz = np.random.rand(filas, columnas)\n",
    "        matrices.append(matriz)\n",
    "    return matrices\n",
    "\n",
    "data = np.array([coord for group in grupos for coord in group]).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "Ws_prueba = [np.array([[1, 1, 1],\n",
    "                       [1, 1, 1],\n",
    "                       [1, 1, 1]]),\n",
    "             np.array([[1, 1, 1, 1],\n",
    "                       [1, 1, 1, 1],])]\n",
    "\n",
    "layers_prueba = [2,3,2]\n",
    "entry_prueba = [1,2]\n",
    "\n",
    "# Ws_prueba = random_weights_gen(layers_prueba)\n",
    "\n",
    "def multi_layer_perceptron(entry, \n",
    "                           layers, \n",
    "                           Ws,\n",
    "                           act_fct = lambda x: x):\n",
    "\n",
    "    Ws_copy = Ws.copy()\n",
    "    layers_values = [np.array(entry).copy()]\n",
    "    hs = []\n",
    "    for k in range(len(layers) - 1):\n",
    "\n",
    "        layers_values[k] = np.append(layers_values[k], -1)\n",
    "\n",
    "        output = np.zeros(layers[k+1])\n",
    "        h = np.zeros(layers[k+1])\n",
    "        for j in range(layers[k+1]):\n",
    "            for i in range(layers[k] + 1):\n",
    "                output[j] += Ws_copy[k][j,i] * layers_values[k][i]\n",
    "            h[j] = output[j]\n",
    "            output[j] = act_fct(h[j])\n",
    "        hs.append(h)\n",
    "        layers_values.append(output.copy())\n",
    "\n",
    "    return layers_values, hs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5., 5.])"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = multi_layer_perceptron(entry=entry_prueba,\n",
    "                                layers=layers_prueba,\n",
    "                                Ws=Ws_prueba)[0]\n",
    "result[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def backpropagation(Ws, \n",
    "                    layers,\n",
    "                    data, \n",
    "                    labels, \n",
    "                    learning_rate,\n",
    "                    act_fct=lambda x: x, \n",
    "                    d_act_fct=lambda x: 1):\n",
    "\n",
    "    Ws_copy = Ws.copy()\n",
    "\n",
    "    for m in range(labels.shape[1]): #labeled examples\n",
    "\n",
    "        layers_values, hs = multi_layer_perceptron(entry=data[:, m], \n",
    "                                                   layers=layers, \n",
    "                                                   Ws=Ws_copy, \n",
    "                                                   act_fct=act_fct)\n",
    "\n",
    "\n",
    "        D = np.zeros(len(layers_values[-1]))\n",
    "        for i in range(len(layers_values[-1])):\n",
    "            D[i] += (layers_values[-1][i] - labels[i, m]) * d_act_fct(hs[-1][i])\n",
    "        \n",
    "        D_ls = [D.copy()]\n",
    "        for i in range(1, len(layers_values)):\n",
    "            D = np.zeros(len(layers_values[-i-1]))\n",
    "            for j in range(len(layers_values[-i-1])):\n",
    "                print(hs[-i-1][j])\n",
    "                print(D_ls[-i])\n",
    "                print(Ws[-i][:,-i])\n",
    "                D[j] += d_act_fct(hs[-i-1][j]) * D_ls[-i] @ Ws[-i][:,-i]\n",
    "            \n",
    "            D_ls.append(D.copy())\n",
    "\n",
    "        D_ls = np.flip(D_ls)\n",
    "\n",
    "        for l in range(len(layers_values) - 1):\n",
    "            for i in range(len(layers_values[l])):\n",
    "                for j in range(len(layers_values[l+1])):\n",
    "\n",
    "                    Ws_copy[l][j,i] -= learning_rate * D_ls[l][j] * layers_values[l][i]\n",
    "\n",
    "    return Ws_copy\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.331925663219762\n",
      "[10.99577699 11.99577699]\n",
      "[1 1]\n",
      "4.331925663219762\n",
      "[10.99577699 11.99577699]\n",
      "[1 1]\n",
      "4.331925663219762\n",
      "[10.99577699 11.99577699]\n",
      "[1 1]\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 3 is out of bounds for axis 0 with size 3",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[195], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m W \u001b[38;5;241m=\u001b[39m \u001b[43mbackpropagation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mWs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mWs_prueba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      2\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      3\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlayers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayers_prueba\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m      5\u001b[0m \u001b[43m                    \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0.02\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[194], line 27\u001b[0m, in \u001b[0;36mbackpropagation\u001b[1;34m(Ws, layers, data, labels, learning_rate, act_fct, d_act_fct)\u001b[0m\n\u001b[0;32m     25\u001b[0m D \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mlen\u001b[39m(layers_values[\u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(layers_values[\u001b[38;5;241m-\u001b[39mi\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m)):\n\u001b[1;32m---> 27\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[43mhs\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mi\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[43mj\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m     28\u001b[0m     \u001b[38;5;28mprint\u001b[39m(D_ls[\u001b[38;5;241m-\u001b[39mi])\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28mprint\u001b[39m(Ws[\u001b[38;5;241m-\u001b[39mi][:,\u001b[38;5;241m-\u001b[39mi])\n",
      "\u001b[1;31mIndexError\u001b[0m: index 3 is out of bounds for axis 0 with size 3"
     ]
    }
   ],
   "source": [
    "W = backpropagation(Ws=Ws_prueba,\n",
    "                    data=data,\n",
    "                    layers=layers_prueba,\n",
    "                    labels=labels,\n",
    "                    learning_rate=0.02)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
